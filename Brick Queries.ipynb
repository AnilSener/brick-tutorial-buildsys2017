{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDF Primer\n",
    "\n",
    "The RDF data model represents a directed, labeled graph as a set of *triples*. A triple consists of a *subject* (node), a *predicate* (directed edge) and an *object* (node). We will interact with the graph by way of these triples\n",
    "\n",
    "<img src=\"files/refresher.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis with Brick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the primary benefits of having a machine-readable representation of a building is the ability to build portable analytics. Rather than hard-coding the specific data streams and points from a building into an analytics script, we can discover that structure dynamically by querying the Brick model.\n",
    "\n",
    "The goal of this part of the tutorial is to demonstrate how to write SPARQL queries that pull information from such graphs, and how to use these queries to write portable analytics.\n",
    "\n",
    "To help guide the queries we want to write, we've rendered a simple visualization of how class instances are connected in our sample building Brick model. The below figure tells us:\n",
    "- nodes of type `brick:VAV` have `feeds` relationships with nodes of type `brick:HVAC_Zone`\n",
    "- nodes of type `brick:Room` have `isPartOf` relationships with nodes of type `brick:HVAC_Zone`\n",
    "\n",
    "<img src=\"files/vavhvac.png\">\n",
    "\n",
    "Here is the full graph:\n",
    "\n",
    "<img src=\"files/classgraph.png\">\n",
    "\n",
    "There are only a few types of relationships (edge labels) that Brick captures, but these are sufficient for what we need. Here's a brief recap:\n",
    "\n",
    "- **Subcomponents**: `hasPart`/`isPartOf`. Used for mechanical composition, relating HVAC zones with physical spaces\n",
    "- **Identifying flows**: `feeds`/`isFedBy`. Used for flows of water, electricity, air, etc.\n",
    "- **Sensing + actuation**: `hasPoint`/`isPointOf`. Used for associating sensor, setpoints, statuses, commands, meters, etc with related equipment, locations, etc\n",
    "- **Location**: `hasLocation`/`isLocationOf`. Identifies the physical location of something\n",
    "- **Instantiation**: `type` (implicit in the graph above)\n",
    "\n",
    "The way we write SPARQL queries is by considering the types of \"things\" we care about as well as the nature of the relationships between those things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we do is set up our query environment. For convenience, we're using the SPARQL engine included in the RDFlib Python library. We provide a brief wrapper around that library for queries, which takes care of most of the boilerplate involved in SPARQL queries. The most important part of this boilerplate are the namespace abbreviations.\n",
    "\n",
    "Recall that in the RDF data model, all nodes are either Literals (strings) or URIs. All URIs have a namespace: for example in the URI `https://brickschema.org/schema/1.0.1/Brick#VAV`, the namespace is `https://brickschema.org/schema/1.0.1/Brick#` and the \"node\" is `VAV`. For convenience, we will abbreviate the URI so we can write `brick:VAV` instead of `https://brickschema.org/schema/1.0.1/Brick#VAV`.\n",
    "\n",
    "Here are the abbreviations we will use:\n",
    "\n",
    "| Namespace | Usage | Abbreviation |\n",
    "|-----------|-------|--------------|\n",
    "| `https://brickschema.org/schema/1.0.1/Brick#` | Brick classes | `brick:` |\n",
    "| `https://brickschema.org/schema/1.0.1/BrickFrame#` | Brick relationships | `bf:` |\n",
    "| `http://www.w3.org/1999/02/22-rdf-syntax-ns#` | class instantiation | `rdf:` |\n",
    "| `http://www.w3.org/2000/01/rdf-schema#` | subclassing | `rdfs:` |\n",
    "| `http://example.com#` | Nodes in our building | `ex:` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from query import BrickGraph\n",
    "bldg = BrickGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARQL Queries\n",
    "\n",
    "## Exploring a Brick Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by executing the 'identity' SPARQL query that lists all of the triples in our model. We have chosen the variable names `?sub` `?pred` `?obj` for clarity; the *position* of the variable within a SPARQL triple conveys its role, rather than the name. For example, because `?obj` is the third item in the SPARQL triple, we know it corresponds to a node in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = bldg.query(\"\"\"select ?sub ?pred ?obj where {\n",
    "?sub ?pred ?obj .\n",
    "}\"\"\", fullURI=False)\n",
    "\n",
    "# print the first 10 rows\n",
    "for row in rows[:10]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the node/edge/node construction in the returned triples.\n",
    "\n",
    "**Listing Instances**\n",
    "\n",
    "Let's amend this query so it starts giving us some more useful information. To start, let's list all of the VAVs in the building. To do this, we want to find all nodes that have a `rdf:type` edge (which indicates instantiation) to the `brick:VAV` node which represents the Brick VAV class. In the corresponding triple, we will put `rdf:type` in the \"predicate\" slot and `brick:VAV` in the \"object\" slot. In the subject slot, we will place a variable that will be populated when the query executes:\n",
    "\n",
    "```sparql\n",
    "?vav rdf:type brick:VAV\n",
    "```\n",
    "\n",
    "This corresponds to finding instances of the following subgraph:\n",
    "\n",
    "<img src=\"files/vavclassq.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = bldg.query(\"\"\"select ?vav where {\n",
    "?vav rdf:type brick:VAV .\n",
    "}\"\"\", fullURI=False)\n",
    "for row in rows:\n",
    "    print('?vav =',row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Listing Neighbors**\n",
    "\n",
    "A natural question when interacting with a new Brick model is what kind of information is associated with a particular VAV. We can express this as a SPARQL query by seeing which triples exist that have a VAV as the subject.\n",
    "\n",
    "```sparql\n",
    "ex:VAV_RM-1100D   ?pred   ?obj \n",
    "```\n",
    "\n",
    "This corresponds to finding instances of the following subgraph:\n",
    "\n",
    "<img src=\"files/vavneighbor.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = bldg.query(\"\"\"select ?pred ?obj where {\n",
    "ex:VAV_RM-2150 ?pred ?obj .\n",
    "}\"\"\", fullURI=False)\n",
    "for row in rows:\n",
    "    print('ex:VAV_RM-2150 has edge', row[0], 'to', row[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Listing Types**\n",
    "\n",
    "The above queries work well if we know the exact classes instantiated in our Brick model. If we don't have this information and want to discover it, we can leverage Brick's class hierarchy.\n",
    "\n",
    "<img src=\"files/tempsensorclass.png\">\n",
    "\n",
    "A common use case is finding which kinds of temperature sensors exist in the Brick model. The basic form of this uses both the `rdf:type` relationship (instantiation) and the `rdfs:subClassOf` relationship. By default, mentioning a predicate only traverses a single edge in the graph. We use SPARQL's `*` operator to match \"0 or more\" instances of an edge to transparently query more than one level of the hierarchy. SPARQL also provides a `+` operator to match \"1 or more\" edges.\n",
    "\n",
    "\n",
    "This matches instances that are *immediate* subclasses of `brick:Temperature_Sensor`\n",
    "```sparql\n",
    "?sensor   rdf:type/rdfs:subClassOf    brick:Temperature_Sensor\n",
    "```\n",
    "\n",
    "This matches instances of `brick:Temperature_Sensor` as well as instances of *any* subclass of `brick:Temperature_Sensor`\n",
    "```sparql\n",
    "?sensor   rdf:type/rdfs:subClassOf*    brick:Temperature_Sensor\n",
    "```\n",
    "\n",
    "This query corresponds to the following subgraph in the case of Zone Air Temperature Sensors\n",
    "\n",
    "<img src=\"files/tempsensorquery.png\">\n",
    "\n",
    "We can now run this query on our real Brick model to see what flavors of Temperature Sensor exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = bldg.query(\"\"\"select ?sensor ?class where {\n",
    "?sensor rdf:type ?class .\n",
    "?class rdfs:subClassOf* brick:Temperature_Sensor .\n",
    "}\"\"\", fullURI=False)\n",
    "for row in rows:\n",
    "    print(row[0], 'is an instance of', row[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are not interested in the intermediate class name, we can combine the expression of the `rdf:type` and `rdfs:subClassOf*` predicates using the `/` SPARQL operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = bldg.query(\"\"\"select ?sensor where {\n",
    "?sensor rdf:type/rdfs:subClassOf* brick:Temperature_Sensor .\n",
    "}\"\"\", fullURI=False)\n",
    "for row in rows:\n",
    "    print(row[0], 'is an instance of a subclass of Temperature_Sensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Cross-Subsystem Queries\n",
    "\n",
    "One of the main benefits of Brick is its ability to represent multiple building subsystems and query across them. We will explore these capabilities of Brick through the implementation of two analytics applications:\n",
    "- Stuck Damper Detection\n",
    "- Simultaneous Heating and Cooling Detection\n",
    "\n",
    "The point of these explorations is not to implement cutting-edge fault detection/diagnosis algorithms, but rather to illustrate how Brick can make it easier to find the relevant data streams and make an implementation portable across buildings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuck Damper Detection\n",
    "\n",
    "One method of detecting stuck dampers is to look at the difference between the supply air flow sensor and setpoint for a VAV.\n",
    "\n",
    "- If the measured air flow is within some delta of the air flow setpoint, then the damper is fine\n",
    "- Else, it might be broken\n",
    "\n",
    "First, we write the Brick query(ies) to extract the relevant data streams: namely, the supply air flow sensor and supply air flow setpoint for each VAV in our building. In addition to the names of those points, we also extract from the Brick model some \"timeseries identifier\" that will point us to where we can obtain the actual data. Here, we use the `brick:hasUuid` relationship which points to an RDF literal corresponding to the name of a CSV data file.\n",
    "\n",
    "Lets progressively build up the query. First, lets begin by identifying all the VAVs in the building:\n",
    "\n",
    "```sparql\n",
    "?vav   rdf:type    brick:VAV .\n",
    "```\n",
    "\n",
    "We will then identify the supply air flow setpoints and sensors using the `rdf:type` relationship, and because they measure an aspect of a VAV, we know they will be related to a VAV using the `isPointOf` relationship.\n",
    "\n",
    "```sparql\n",
    "?setpoint   rdf:type     brick:Supply_Air_Flow_Setpoint .\n",
    "?sensor     rdf:type     brick:Supply_Air_Flow_Sensor .\n",
    "?setpoint   bf:isPointOf brick:VAV .\n",
    "?sensor     bf:isPointOf brick:VAV .\n",
    "```\n",
    "\n",
    "Lastly, we pull out the UUIDs for the timeseries\n",
    "\n",
    "```sparql\n",
    "?setpoint   brick:hasUuid  ?setpoint_uuid .\n",
    "?sensor     brick:hasUuid  ?sensor_uuid .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = bldg.query(\"\"\"select ?vav ?sensor ?setpoint ?sensor_uuid ?setpoint_uuid where {\n",
    "?vav       rdf:type     brick:VAV .\n",
    "?setpoint  rdf:type     brick:Supply_Air_Flow_Setpoint .\n",
    "?sensor    rdf:type     brick:Supply_Air_Flow_Sensor .\n",
    "?setpoint  bf:isPointOf ?vav .\n",
    "?sensor  bf:isPointOf ?vav .\n",
    "?setpoint   brick:hasUuid  ?setpoint_uuid .\n",
    "?sensor     brick:hasUuid  ?sensor_uuid .\n",
    "}\"\"\", fullURI=False)\n",
    "\n",
    "# build a simple associative structure for the VAV points\n",
    "vavs = {}\n",
    "for row in rows:\n",
    "    vavs[row[0]] = {'setpoint': row[4], 'sensor': row[3]}\n",
    "print(vavs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 100\n",
    "for vavname, points in vavs.items():\n",
    "    setpoint_df = pd.read_csv('data/{0}.csv'.format(points['setpoint']), usecols=['timestamp','value'])\n",
    "    sensor_df = pd.read_csv('data/{0}.csv'.format(points['sensor']), usecols=['timestamp','value'])\n",
    "    \n",
    "    # parse timestamps\n",
    "    setpoint_df = setpoint_df.set_index(pd.to_datetime(setpoint_df.pop('timestamp')))\n",
    "    sensor_df = sensor_df.set_index(pd.to_datetime(sensor_df.pop('timestamp')))\n",
    "    \n",
    "    # resample 10-min mean\n",
    "    setpoint_df = setpoint_df.resample('10T').mean()\n",
    "    sensor_df = sensor_df.resample('10T').mean()\n",
    "    \n",
    "    # join the dataframes together so we can compare the values easier\n",
    "    vav_df = setpoint_df.join(sensor_df, lsuffix='_setpoint', rsuffix='_sensor')\n",
    "    ax = vav_df.plot(figsize=(15,10))\n",
    "    ax.set_title('{0} Supply Air Flow/Setpoint'.format(vavname))\n",
    "    ax.set_ylabel('Supply Air Flow (cfm)')\n",
    "    \n",
    "    # find datapoints where the difference between setpoint and sensor is greater than the threshold\n",
    "    broken = vav_df[(vav_df['value_setpoint'] - vav_df['value_sensor']).abs() > THRESHOLD]\n",
    "    if len(broken) > 0:\n",
    "        print(vavname, 'has faults!!')\n",
    "        broken.pop('value_setpoint')\n",
    "        broken.columns = ['Broken Damper']\n",
    "        broken.plot(style='ro',ax=ax)\n",
    "    else:\n",
    "        print(vavname,'has no faults')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simultaneous Heating and Cooling Detection\n",
    "\n",
    "Properly identifying simultaneous heating and cooling in a building involves traversing the HVAC and spatial hierarchies of the building. We must first find rooms that are contained within more than one HVAC zone and therefore are conditioned by more than one VAV.\n",
    "\n",
    "From this, we have a few possible avenues. First, we can compare the reheat coil percentages for the VAVs feeding a single zone. Additionally, we can look at the cooling coil percentage for the AHU upstream of each VAV combined with the supply air flow as further evidence.\n",
    "\n",
    "To begin, we need to find rooms that are in more than one HVAC zone. We follow the same procedure for building up the query: identify the instances of the relevant classes, and then filter these by the required relationships.\n",
    "\n",
    "The instances we are interested in for this first queries are rooms and HVAC zones:\n",
    "\n",
    "```sparql\n",
    "?room    rdf:type    brick:Room .\n",
    "?zone    rdf:type    brick:HVAC_Zone .\n",
    "```\n",
    "\n",
    "We then relate rooms and HVAC zones using the proper `isPartOf` relationship:\n",
    "\n",
    "```sparql\n",
    "?room   bf:isPartOf  ?zone .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = bldg.query(\"\"\"select ?zone ?room where {\n",
    "?room rdf:type brick:Room .\n",
    "?zone rdf:type brick:HVAC_Zone .\n",
    "?room bf:isPartOf ?zone .\n",
    "}\"\"\", fullURI=False)\n",
    "\n",
    "# build a simple associative structure for the room -> zone mapping\n",
    "rooms = defaultdict(list)\n",
    "for row in rows:\n",
    "     rooms[row[1]].append(row[0])\n",
    "\n",
    "rooms_to_examine = []\n",
    "for room, zonelist in rooms.items():\n",
    "    if len(zonelist) > 1:\n",
    "        print('Room {0} has {1} zones'.format(room, len(zonelist)))\n",
    "        rooms_to_examine.append(room)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For each of the rooms in multiple HVAC zones, we get the reheat coil valve command for the VAVs feeding those zones. Following the rules of composition for the Brick model, we know there is:\n",
    "\n",
    "- an `isPartOf` relationship from the room to the zone\n",
    "- a `feeds` relationship from the VAV to the zone\n",
    "- a `isPointOf` relationship from the reheat valve command to the VAV\n",
    "\n",
    "We combine these into the following SPARQL triples:\n",
    "\n",
    "```sparql\n",
    "?zone    rdf:type    brick:HVAC_Zone .\n",
    "?vav     rdf:type    brick:VAV .\n",
    "?rhc     rdf:type    brick:Reheat_Valve_Command .\n",
    "\n",
    "<our room>  bf:isPartOf    ?zone .\n",
    "?vav        bf:feeds       ?zone .\n",
    "?rhc        bf:isPointOf   ?vav .\n",
    "?rhc        brick:hasUuid  ?rhc_uuid .\n",
    "```\n",
    "\n",
    "The last triple gets us the UUID for the timeseries data representing the actual reheat valve command values.\n",
    "\n",
    "We add to our query the Supply Air Flow sensors so we can tell how much hot/cold air is being blown into a room from the different VAVs\n",
    "\n",
    "```sparql\n",
    "?saf    rdf:type      brick:Supply_Air_Flow_Sensor .\n",
    "?saf    bf:isPointOf  ?vav .\n",
    "?saf    brick:hasUuid ?saf_uuid .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RHC_THRESHOLD = 50\n",
    "for room in rooms_to_examine:\n",
    "    rows = bldg.query(\"\"\"select ?vav ?rhc_uuid ?saf_uuid where {{\n",
    "    ?zone    rdf:type    brick:HVAC_Zone .\n",
    "    ?vav     rdf:type    brick:VAV .\n",
    "    ?rhc     rdf:type    brick:Reheat_Valve_Command .\n",
    "    ?saf     rdf:type    brick:Supply_Air_Flow_Sensor .\n",
    "    {0} bf:isPartOf    ?zone .\n",
    "    ?vav        bf:feeds       ?zone .\n",
    "    ?rhc        bf:isPointOf   ?vav .\n",
    "    ?rhc        brick:hasUuid  ?rhc_uuid .\n",
    "    ?saf        bf:isPointOf   ?vav .\n",
    "    ?saf        brick:hasUuid  ?saf_uuid .\n",
    "    }}\"\"\".format(room), fullURI=False)\n",
    "    print(rows)\n",
    "    rhc1 = pd.read_csv('data/{0}.csv'.format(rows[0][1]), usecols=['timestamp','value'])\n",
    "    flow1 = pd.read_csv('data/{0}.csv'.format(rows[0][2]), usecols=['timestamp','value'])\n",
    "\n",
    "    rhc2 = pd.read_csv('data/{0}.csv'.format(rows[1][1]), usecols=['timestamp','value'])\n",
    "    flow2 = pd.read_csv('data/{0}.csv'.format(rows[1][2]), usecols=['timestamp','value'])\n",
    "\n",
    "    # parse timestamps\n",
    "    rhc1 = rhc1.set_index(pd.to_datetime(rhc1.pop('timestamp'))).resample('10T').mean()\n",
    "    flow1 = flow1.set_index(pd.to_datetime(flow1.pop('timestamp'))).resample('10T').mean()\n",
    "    rhc2 = rhc2.set_index(pd.to_datetime(rhc2.pop('timestamp'))).resample('10T').mean()\n",
    "    flow2 = flow2.set_index(pd.to_datetime(flow2.pop('timestamp'))).resample('10T').mean()\n",
    "    \n",
    "    # join the dataframes together so we can compare the values easier\n",
    "    rhc = rhc1.join(rhc2, lsuffix='_rhc1', rsuffix='_rhc2')\n",
    "    flow = flow1.join(flow2, lsuffix='_flow1', rsuffix='_flow2')\n",
    "    #rhc.plot(figsize=(15,10))\n",
    "    #flow.plot(figsize=(15,10))\n",
    "    \n",
    "    # find ranges where the reheat coil commands are more than RHC_THRESHOLD away \n",
    "    # from each other and the supply air flow is non-zero for both\n",
    "    diff_rhc_values = ((rhc['value_rhc1'] - rhc['value_rhc2']).abs() >= RHC_THRESHOLD)\n",
    "    non_zero_saf = ((flow['value_flow1'] > 50) & (flow['value_flow2'] > 50))\n",
    "    ax = flow[(diff_rhc_values & non_zero_saf)].plot(figsize=(15,10))\n",
    "    ax.set_title('Possible Simultaneous Heating/Cooling')\n",
    "    ax.set_ylabel('Air Flow (cfm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of looking at the simultaneous heating/cooling problem is by comparing the heating/cooling setpoints for the two VAVs. Similar to how we ask the Brick model for the reheat coil command and supply air flow temperature sensor, we can also ask the Brick model for the heating/cooling setpoints for the zone.\n",
    "\n",
    "The building we're working with exposes the *effective* setpoint rather than the *actual* setpoint, so we need to adjust our query from asking for instances of `Heating Temperature Setpoint` to instances of *subclasses* of `Heating Temperature Setpoint`. A sufficiently advanced application might treat effective setpoints differently, but for our basic analysis, we will make this conflation.\n",
    "\n",
    "Below, we construct the Brick query to obtain this information, and then compute the \"effective deadband\", which is the actual range of temperatures allowed before one of the VAVs starts operating. As we can see from the data, the effective deadband is only about 1 degree Fahrenheit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for room in rooms_to_examine:\n",
    "    rows = bldg.query(\"\"\"select ?vav ?hsp_uuid ?csp_uuid where {{\n",
    "    ?zone    rdf:type    brick:HVAC_Zone .\n",
    "    ?vav     rdf:type    brick:VAV .\n",
    "    ?hsp     rdf:type/rdfs:subClassOf*    brick:Heating_Temperature_Setpoint .\n",
    "    ?csp     rdf:type/rdfs:subClassOf*    brick:Cooling_Temperature_Setpoint .\n",
    "    {0} bf:isPartOf    ?zone .\n",
    "    ?vav        bf:feeds       ?zone .\n",
    "    ?hsp        bf:isPointOf   ?vav .\n",
    "    ?hsp        brick:hasUuid  ?hsp_uuid .\n",
    "    ?csp        bf:isPointOf   ?vav .\n",
    "    ?csp        brick:hasUuid  ?csp_uuid .\n",
    "    }}\"\"\".format(room), fullURI=False)\n",
    "    print(rows)\n",
    "    vav1_hsp = pd.read_csv('data/{0}.csv'.format(rows[0][1]), usecols=['timestamp','value'])\n",
    "    vav1_csp = pd.read_csv('data/{0}.csv'.format(rows[0][2]), usecols=['timestamp','value'])\n",
    "    vav2_hsp = pd.read_csv('data/{0}.csv'.format(rows[1][1]), usecols=['timestamp','value'])\n",
    "    vav2_csp = pd.read_csv('data/{0}.csv'.format(rows[1][2]), usecols=['timestamp','value'])\n",
    "\n",
    "    \n",
    "    # parse timestamps + resample\n",
    "    vav1_hsp = vav1_hsp.set_index(pd.to_datetime(vav1_hsp.pop('timestamp'))).resample('10T').mean()\n",
    "    vav1_csp = vav1_csp.set_index(pd.to_datetime(vav1_csp.pop('timestamp'))).resample('10T').mean()\n",
    "    vav2_hsp = vav2_hsp.set_index(pd.to_datetime(vav2_hsp.pop('timestamp'))).resample('10T').mean()\n",
    "    vav2_csp = vav2_csp.set_index(pd.to_datetime(vav2_csp.pop('timestamp'))).resample('10T').mean()\n",
    "    vav1_hsp.columns=['hsp']\n",
    "    vav2_hsp.columns=['hsp']\n",
    "    vav1_csp.columns=['csp']\n",
    "    vav2_csp.columns=['csp']\n",
    "    \n",
    "    # join the dataframes together so we can compare the values easier\n",
    "    hsp = vav1_hsp.join(vav2_hsp, lsuffix='_vav1', rsuffix='_vav2')\n",
    "    ax = hsp.plot(style=['r-', 'r--'], figsize=(15,10))\n",
    "    csp = vav1_csp.join(vav2_csp, lsuffix='_vav1', rsuffix='_vav2')\n",
    "    csp.plot(style=['b-','b--'], figsize=(15,10), ax=ax)\n",
    "    ax.set_title('All deadbands')\n",
    "    \n",
    "    # compute effective deadband\n",
    "    effective = pd.DataFrame({'hsp': hsp.max(axis=1), 'csp': csp.min(axis=1)})\n",
    "    ax2= effective.plot(style=['r-','b-'], figsize=(15,10))\n",
    "    ax2.set_title('Effective deadband')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
